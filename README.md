# Audio-FLAN
**Audio-FLAN** aims to **unify audio-language models** that can seamlessly handle both **understanding** and **generation** tasks across **speech, music, sound**.

## Audio-FLAN: A Preliminary Release
<div>
    <a href="https://huggingface.co/HKUSTAudio"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Huaging_Face-HKUST_Audio-yellow"></a>
    <a href="https://arxiv.org/pdf/2502.16584"><img src="https://img.shields.io/badge/arxiv-Audio-FLAN-green">
</a>
</div>
<br>

**An Instruction-Tuning Dataset for Unified Audio Understanding and Generation Across Speech, Music, and Sound**


Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly **unified** audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering **80 diverse tasks** across **speech, music, and sound** domains, with **over 100 million instances**. **Audio-FLAN** lays the foundation for unified audio-language models that can seamlessly handle both **understanding** (e.g., transcription, comprehension) and **generation** (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The **Audio-FLAN** dataset is available on [HuggingFace](https://huggingface.co/HKUSTAudio) and [GitHub](https://github.com/lmxue/Audio-FLAN) and will be continuously updated.


<!-- ## Updates
## Citation -->

